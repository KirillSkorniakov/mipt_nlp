{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import normalize, scale\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.feature_extraction.text import _document_frequency\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"../data/SMSSpamCollection\"\n",
    "\n",
    "with open(data_path) as f:\n",
    "    data = pd.DataFrame([{\"isSpam\": line.split(\"\\t\")[0],\n",
    "                         \"text\": line.split(\"\\t\")[1]} for line in f ])\n",
    "data[\"label\"] = LabelEncoder().fit_transform(data[\"isSpam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformer = CountVectorizer()\n",
    "X = transformer.fit_transform(data[\"text\"])\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score: 0.933348526858\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X, data[\"label\"], cv=10, n_jobs=-1, scoring=\"f1\")\n",
    "print(\"average score:\", np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers:  1 1 0 0 0\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "samples = [\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\",\n",
    "           \"FreeMsg: Txt: claim your reward of 3 hours talk time\",\n",
    "           \"Have you visited the last lecture on physics?\",\n",
    "           \"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\",\n",
    "           \"Only 99$\"]\n",
    "results = clf.predict(transformer.transform(samples))\n",
    "print(\"answers: \", \" \".join(results.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score: 0.822422066419\n",
      "average score: 0.725016155547\n",
      "average score: 0.925138255865\n"
     ]
    }
   ],
   "source": [
    "n_gramm_list = [(2, 2), (3,3), (1, 3)]\n",
    "for n_gramm in n_gramm_list:\n",
    "    transformer = CountVectorizer(ngram_range=n_gramm)\n",
    "    X = transformer.fit_transform(data[\"text\"])\n",
    "    y = data[\"label\"]\n",
    "    scores = cross_val_score(LogisticRegression(), X, data[\"label\"], cv=10, n_jobs=-1, scoring=\"f1\")\n",
    "    print(\"average score:\", np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score: 0.645455401356\n",
      "average score: 0.378623430876\n",
      "average score: 0.887905460889\n"
     ]
    }
   ],
   "source": [
    "n_gramm_list = [(2, 2), (3,3), (1, 3)]\n",
    "for n_gramm in n_gramm_list:\n",
    "    transformer = CountVectorizer(ngram_range=n_gramm)\n",
    "    X = transformer.fit_transform(data[\"text\"])\n",
    "    y = data[\"label\"]\n",
    "    scores = cross_val_score(MultinomialNB(), X, data[\"label\"], cv=10, n_jobs=-1, scoring=\"f1\")\n",
    "    print(\"average score:\", np.average(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score: 0.840253457542\n"
     ]
    }
   ],
   "source": [
    "transformer = TfidfVectorizer()\n",
    "X = transformer.fit_transform(data[\"text\"])\n",
    "y = data[\"label\"]\n",
    "scores = cross_val_score(MultinomialNB(), X, data[\"label\"], cv=10, n_jobs=-1, scoring=\"f1\")\n",
    "print(\"average score:\", np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf работает хуже, попробуем bm25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# взято отсюда\n",
    "# https://github.com/alexeygrigorev/avito-duplicates-kaggle/blob/master/bm25.py\n",
    "class BM25Transformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    use_idf : boolean, optional (default=True)\n",
    "    k1 : float, optional (default=2.0)\n",
    "    b  : float, optional (default=0.75)\n",
    "    References\n",
    "    ----------\n",
    "    Okapi BM25: a non-binary model - Introduction to Information Retrieval\n",
    "    http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html\n",
    "    \"\"\"\n",
    "    def __init__(self, use_idf=True, k1=2.0, b=0.75):\n",
    "        self.use_idf = use_idf\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : sparse matrix, [n_samples, n_features] document-term matrix\n",
    "        \"\"\"\n",
    "        if not sp.issparse(X):\n",
    "            X = sp.csc_matrix(X)\n",
    "        if self.use_idf:\n",
    "            n_samples, n_features = X.shape\n",
    "            df = _document_frequency(X)\n",
    "            idf = np.log((n_samples - df + 0.5) / (df + 0.5))\n",
    "            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features, n=n_features)\n",
    "\n",
    "        doc_len = X.sum(axis=1)\n",
    "        self._average_document_len = np.average(doc_len)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, copy=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : sparse matrix, [n_samples, n_features] document-term matrix\n",
    "        copy : boolean, optional (default=True)\n",
    "        \"\"\"\n",
    "        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
    "            # preserve float family dtype\n",
    "            X = sp.csr_matrix(X, copy=copy)\n",
    "        else:\n",
    "            # convert counts or binary occurrences to floats\n",
    "            X = sp.csr_matrix(X, dtype=np.float, copy=copy)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Document length (number of terms) in each row\n",
    "        # Shape is (n_samples, 1)\n",
    "        doc_len = X.sum(axis=1)\n",
    "        # Number of non-zero elements in each row\n",
    "        # Shape is (n_samples, )\n",
    "        sz = X.indptr[1:] - X.indptr[0:-1]\n",
    "\n",
    "        # In each row, repeat `doc_len` for `sz` times\n",
    "        # Shape is (sum(sz), )\n",
    "        # Example\n",
    "        # -------\n",
    "        # dl = [4, 5, 6]\n",
    "        # sz = [1, 2, 3]\n",
    "        # rep = [4, 5, 5, 6, 6, 6]\n",
    "        rep = np.repeat(np.asarray(doc_len), sz)\n",
    "\n",
    "        # Compute BM25 score only for non-zero elements\n",
    "        nom = self.k1 + 1\n",
    "        denom = X.data + self.k1 * (1 - self.b + self.b * rep / self._average_document_len)\n",
    "        data = X.data * nom / denom\n",
    "\n",
    "        X = sp.csr_matrix((data, X.indices, X.indptr), shape=X.shape)\n",
    "\n",
    "        if self.use_idf:\n",
    "            check_is_fitted(self, '_idf_diag', 'idf vector is not fitted')\n",
    "\n",
    "            expected_n_features = self._idf_diag.shape[0]\n",
    "            if n_features != expected_n_features:\n",
    "                raise ValueError(\"Input has n_features=%d while the model\"\n",
    "                                 \" has been trained with n_features=%d\" % (\n",
    "                                     n_features, expected_n_features))\n",
    "            X = X * self._idf_diag\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X, copy=True):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score: 0.943114179038\n"
     ]
    }
   ],
   "source": [
    "transformer = CountVectorizer(ngram_range=(1, 1))\n",
    "bm25 = BM25Transformer(b=0).fit(transformer.fit_transform(data[\"text\"]))\n",
    "X = bm25.transform(transformer.fit_transform(data[\"text\"]))\n",
    "y = data[\"label\"]\n",
    "scores = cross_val_score(LogisticRegression(), X, data[\"label\"], cv=10, n_jobs=-1, scoring=\"f1\")\n",
    "print(\"average score:\", np.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подберем оптимальное C и регуляризацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1024 tasks      | elapsed:   11.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 0.18329807108324356, 'clf__class_weight': {0: 0.1340150699677072, 1: 0.8659849300322928}, 'transformer__b': 0.0, 'transformer__k1': 4.0}\n",
      "0.956666702282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1800 out of 1800 | elapsed:   20.7s finished\n"
     ]
    }
   ],
   "source": [
    "estimators = [(\"transformer\", BM25Transformer()), (\"clf\", LogisticRegression())]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "transformer = CountVectorizer()\n",
    "\n",
    "X = transformer.fit_transform(data[\"text\"])\n",
    "y = data[\"label\"]\n",
    "class_weight = {0: (len(y) - y[y == 0].shape[0]) / len(y),\n",
    "               1: (len(y) - y[y == 1].shape[0]) / len(y)}\n",
    "params = {\"clf__C\": np.logspace(-1, 4, 20), \n",
    "          \"clf__class_weight\": [class_weight],\n",
    "          \"transformer__b\": np.linspace(0, 1, 3),\n",
    "         \"transformer__k1\": np.linspace(1, 4, 3)}\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42).split(X, y)\n",
    "grid = GridSearchCV(pipe, params, n_jobs=-1, cv=cv, scoring=\"f1\", verbose=1)\n",
    "grid.fit(X, y)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попробуем убрать стоп-слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "{'clf__C': 0.10000000000000001, 'clf__class_weight': {0: 0.1340150699677072, 1: 0.8659849300322928}, 'transformer__b': 0, 'transformer__k1': 4}\n",
      "0.951628130087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "transformer = CountVectorizer(stop_words=\"english\")\n",
    "X = transformer.fit_transform(data[\"text\"])\n",
    "y = data[\"label\"]\n",
    "estimators = [(\"transformer\", BM25Transformer()), (\"clf\", LogisticRegression())]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "class_weight = {0: (len(y) - y[y == 0].shape[0]) / len(y),\n",
    "               1: (len(y) - y[y == 1].shape[0]) / len(y)}\n",
    "params = {\"clf__C\": np.logspace(-1, 4, 20), \n",
    "          \"clf__class_weight\": [class_weight],\n",
    "          \"transformer__b\": [0],\n",
    "          \"transformer__k1\": [4]}\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42).split(X, y)\n",
    "grid = GridSearchCV(pipe, params, n_jobs=-1, cv=cv, scoring=\"f1\", verbose=1)\n",
    "grid.fit(X, y)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добавим лемматизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(reduce_len=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join(map(lemmatizer.lemmatize, tokenizer.tokenize(text)))\n",
    "\n",
    "tokenizer = TweetTokenizer(reduce_len=True)\n",
    "data[\"lemma_text\"] = data[\"text\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': {0: 0.1340150699677072, 1: 0.8659849300322928}, 'C': 263.66508987303581, 'penalty': 'l2'}\n",
      "0.949127503225\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "{'clf__C': 0.18329807108324356, 'clf__class_weight': {0: 0.1340150699677072, 1: 0.8659849300322928}, 'transformer__b': 0, 'transformer__k1': 4}\n",
      "0.957399436005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "transformer = CountVectorizer()\n",
    "X = transformer.fit_transform(data[\"lemma_text\"])\n",
    "y = data[\"label\"]\n",
    "\n",
    "params = {\"C\": np.logspace(-1, 4, 20), \"penalty\": [\"l1\", \"l2\"], \"class_weight\": [class_weight]}\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42).split(X, y)\n",
    "grid = GridSearchCV(LogisticRegression(), params, n_jobs=-1, cv=cv, scoring=\"f1\", verbose=1)\n",
    "grid.fit(X, y)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "transformer = CountVectorizer()\n",
    "X = transformer.fit_transform(data[\"lemma_text\"])\n",
    "y = data[\"label\"]\n",
    "estimators = [(\"transformer\", BM25Transformer()), (\"clf\", LogisticRegression())]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "params = {\"clf__C\": np.logspace(-1, 4, 20), \n",
    "          \"clf__class_weight\": [class_weight],\n",
    "          \"transformer__b\": [0],\n",
    "          \"transformer__k1\": [4]}\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42).split(X, y)\n",
    "grid = GridSearchCV(pipe, params, n_jobs=-1, cv=cv, scoring=\"f1\", verbose=1)\n",
    "grid.fit(X, y)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добавим еще признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_replce(text):\n",
    "    return \" \".join([\"url\" if (\"http\" in word) or (\"www\" in word) else word for word in text.split()])\n",
    "\n",
    "def url_count(text):\n",
    "    url_regexp = re.compile(\"(https?:\\\\/\\\\/)?(www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{2,256}\\\\.[a-z]{2,6}\\\\b([-a-zA-Z0-9@:%_\\\\+.~#?&//=]*)\")\n",
    "    #return len(re.findall(url_regexp, text))\n",
    "    return 1 if (\"http\" in text) or (\"www\" in text) else 0\n",
    "\n",
    "def capital_letters_range(text):\n",
    "    return len(list(filter(lambda x: x.isupper(), text))) / len(text)\n",
    "\n",
    "def num_count(text):\n",
    "    num_regexp = re.compile(\"[0-9]+\\.?[0-9]*\")\n",
    "    return len(re.findall(num_regexp, text))\n",
    "\n",
    "def money_count(text):\n",
    "    money_regexp = re.compile(\"[$£]+\")\n",
    "    return len(re.findall(money_regexp, text))\n",
    "\n",
    "def citation_count(text):\n",
    "    money_regexp = re.compile('\\\"+')\n",
    "    return len(re.findall(money_regexp, text))\n",
    "\n",
    "def capital_words_count(text):\n",
    "    return len(list(filter(lambda x: x.isupper(), tokenizer.tokenize(text))))\n",
    "\n",
    "def phone_number_count(text):\n",
    "    num_regexp = re.compile(\"[0-9]+\\.?[0-9]*\")\n",
    "    return len(list(filter(lambda x: len(x) > 4, re.findall(num_regexp, text))))\n",
    "\n",
    "tokenizer = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "data[\"text_url\"] = data[\"lemma_text\"].apply(url_replce)\n",
    "data[\"url_count\"] = data[\"text\"].apply(url_count)\n",
    "data[\"chars_len\"] = data[\"text\"].apply(lambda text: len(text))\n",
    "data[\"words_len\"] = data[\"text\"].apply(lambda text: len(tokenizer.tokenize(text)))\n",
    "data[\"capital_letters_range\"] = data[\"text\"].apply(capital_letters_range)\n",
    "data[\"num_count\"] = data[\"text\"].apply(num_count)\n",
    "data[\"money_count\"] = data[\"text\"].apply(money_count)\n",
    "data[\"capital_words_count\"] = data[\"text\"].apply(capital_words_count)\n",
    "data[\"phone_number_count\"] = data[\"text\"].apply(phone_number_count)\n",
    "data[\"citation_count\"] = data[\"text\"].apply(citation_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n",
      "{'class_weight': {0: 0.1340150699677072, 1: 0.8659849300322928}, 'C': 42.813323987193911, 'penalty': 'l2'}\n",
      "0.964921457782\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:   12.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 0.088586679041008226, 'clf__class_weight': {0: 0.1340150699677072, 1: 0.8659849300322928}, 'transformer__b': 0, 'transformer__k1': 4}\n",
      "0.966008523135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    2.6s finished\n"
     ]
    }
   ],
   "source": [
    "transformer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = transformer.fit_transform(data[\"text\"])\n",
    "columns = [\"url_count\", \"words_len\", \"capital_letters_range\", \"num_count\",\n",
    "          \"money_count\", \"capital_words_count\", \"phone_number_count\"]\n",
    "X_features = csr_matrix(data[columns].values)\n",
    "X_joined = hstack((X, X_features))\n",
    "y = data[\"label\"]\n",
    "class_weight = {0: (len(y) - y[y == 0].shape[0]) / len(y),\n",
    "               1: (len(y) - y[y == 1].shape[0]) / len(y)}\n",
    "params = {\"C\": np.logspace(-1, 4, 20), \"penalty\": [\"l1\", \"l2\"], \"class_weight\": [class_weight]}\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42).split(X, y)\n",
    "grid = GridSearchCV(LogisticRegression(), params, cv=cv, scoring=\"f1\", verbose=1)\n",
    "grid.fit(X_joined, y)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "\n",
    "transformer = CountVectorizer()\n",
    "#X = BM25Transformer(b=0, k1=4).fit_transform(transformer.fit_transform(data[\"text\"]))\n",
    "X = transformer.fit_transform(data[\"text\"])\n",
    "y = data[\"label\"]\n",
    "X_joined = hstack((X, X_features))\n",
    "estimators = [(\"transformer\", BM25Transformer()), (\"clf\", LogisticRegression())]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "params = {\"clf__C\": np.logspace(-2, 4, 20), \n",
    "          \"clf__class_weight\": [class_weight],\n",
    "          \"transformer__b\": [0],\n",
    "          \"transformer__k1\": [4]}\n",
    "#params = {\"C\": np.logspace(-2, 4, 20), \"class_weight\": [class_weight]}\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42).split(X, y)\n",
    "grid = GridSearchCV(pipe, params, n_jobs=-1, cv=cv, scoring=\"f1\", verbose=1)\n",
    "grid.fit(X_joined, y)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь попробуем деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   15.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'colsample_bytree': 0.4, 'colsample_bylevel': 0.6, 'min_child_weight': 1, 'n_estimators': 500}\n",
      "0.964032355569\n"
     ]
    }
   ],
   "source": [
    "transformer = CountVectorizer(ngram_range=(1, 1))\n",
    "X = transformer.fit_transform(data[\"text\"])\n",
    "columns = [\"url_count\", \"words_len\", \"capital_letters_range\", \"num_count\",\n",
    "          \"money_count\", \"capital_words_count\", \"phone_number_count\"]\n",
    "X_features = csr_matrix(data[columns].values)\n",
    "X_joined = hstack((X, X_features))\n",
    "y = data[\"label\"]\n",
    "\n",
    "params = {\"n_estimators\": [500],\n",
    "          \"colsample_bytree\": [0.4], \n",
    "          \"colsample_bylevel\": [0.6],\n",
    "          \"min_child_weight\": [1],\n",
    "          \"max_depth\": [3]}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42).split(X, y)\n",
    "grid = GridSearchCV(XGBClassifier(), params, cv=cv, scoring=\"f1\", verbose=1)\n",
    "grid.fit(X_joined, y)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попробуем усреднить результаты предсказаний деревьев и линейного классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.929577464789\n",
      "0.972602739726\n",
      "0.958333333333\n",
      "0.958904109589\n",
      "0.96644295302\n",
      "0.965517241379\n",
      "0.931506849315\n",
      "0.965517241379\n",
      "0.986486486486\n",
      "0.942857142857\n",
      "cv score:  0.957774556187\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "transformer = CountVectorizer()\n",
    "X = transformer.fit_transform(data[\"text\"])\n",
    "columns = [\"url_count\", \"words_len\", \"capital_letters_range\", \"num_count\",\n",
    "          \"money_count\", \"capital_words_count\", \"phone_number_count\"]\n",
    "X_features = csr_matrix(data[columns].values)\n",
    "X_joined = hstack((X, X_features), format=\"csr\")\n",
    "\n",
    "y = data[\"label\"]\n",
    "\n",
    "cv_score = np.zeros(10)\n",
    "\n",
    "for number, (train_index, test_index) in enumerate(cv.split(X, y)):\n",
    "    X_joined_train, X_joined_test = X_joined[train_index, :], X_joined[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    estimators = [(\"transformer\", BM25Transformer()), (\"clf\", LogisticRegression())]\n",
    "    pipe = Pipeline(estimators)\n",
    "    pipe.set_params(clf__C=0.0885, clf__class_weight=class_weight, transformer__b=0, transformer__k1=4)\n",
    "    clf_1 = pipe\n",
    "    clf_1.fit(X_joined_train, y_train)\n",
    "    result_1 = clf_1.predict_proba(X_joined_test)\n",
    "\n",
    "    clf_2 = XGBClassifier(n_estimators=500, colsample_bytree=0.4, colsample_bylevel=0.6, min_child_weight=1,\n",
    "                         max_depth=3)\n",
    "    clf_2.fit(X_joined_train, y_train)\n",
    "    result_2 = clf_2.predict_proba(X_joined_test)\n",
    "    \n",
    "    result = np.argmax(result_1 + result_2, axis=1)\n",
    "    print(f1_score(y_test, result))\n",
    "    cv_score[number] = f1_score(y_test, result)\n",
    "print(\"cv score: \", cv_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы\n",
    "На коротких зашумленных текстах tf-idf работает не очень, но можно настроить bm25.\n",
    "\n",
    "Помогают дополнительные признаки, такие как количество телефонных номеров в тексте, url'ов.\n",
    "\n",
    "Деревья и линейных методы работают почти одинаково неплохо, их усреднение работает еще лучше, неплохо работал KNN на дополнительных признаках. \n",
    "\n",
    "Поэтому можно делать ансамбль, но сэпл маленький и делать без отложенного контроля не очень хочется.\n",
    "\n",
    "Лемматизация не особо помогала."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def url_count(text):\n",
    "    url_regexp = re.compile(\"(https?:\\\\/\\\\/)?(www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{2,256}\\\\.[a-z]{2,6}\\\\b([-a-zA-Z0-9@:%_\\\\+.~#?&//=]*)\")\n",
    "    #url_regexp = re.compile(\"(www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{2,256}\\\\.[a-z]{2,6}\\\\b([-a-zA-Z0-9@:%_\\\\+.~#?&//=]*)\")\n",
    "    return len(re.findall(url_regexp, text))\n",
    "\n",
    "text = \"www.bigfick.com\"\n",
    "print(url_count(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
